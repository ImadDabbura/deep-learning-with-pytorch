{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40263  352929 2025486 ../data/anna.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc ../data/anna.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985223"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "with open('../data/anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 100 characters\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: '8', 1: 'N', 2: 'C', 3: 'm', 4: 'F'},\n",
       " {'8': 0, 'N': 1, 'C': 2, 'm': 3, 'F': 4})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create char_to_idx and idx_to_char dictionaries\n",
    "chars = list(set(text))\n",
    "idx_to_char = dict(enumerate(chars))\n",
    "char_to_idx = {ch: i for i, ch in idx_to_char.items()}\n",
    "\n",
    "{k:v for (k, v) in list(idx_to_char.items())[:5]},\\\n",
    "{k:v for (k, v) in list(char_to_idx.items())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters : 83.\n",
      "Total number of unique characters : 1985223.\n"
     ]
    }
   ],
   "source": [
    "# Encode the text -- convert each char from str to int\n",
    "encoded_text = np.array([char_to_idx[ch] for ch in text])\n",
    "\n",
    "print(f'Total number of characters : {len(chars)}.')\n",
    "print(f'Total number of unique characters : {len(text)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 19, 66, 44,  5, 11, 63, 61, 50, 23])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    '''Convert each element of `arr` into one-hot encoding vector.'''\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels))\n",
    "\n",
    "    # Fill the corresponding idx of each char with 1 - vectorized version\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`n_labels` is usually called the vocabulary size in NLP literature. The vocabulary here is all the unique characters in the text. So the output for each character will be a probability distribution over all these unique characters where the one with the highest probability will be the character that is most likely to be at that time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 83)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = one_hot_encode(np.array([[1, 2], [3, 4]]), len(chars))\n",
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To train on this data, we also want to create mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "We will be creating mini-batched such that the array of encoded text will be of size `batch_size x (seq_length *  total_batches`). In other words, the encoded text will be reshaped so that first dimension would be `batch_size` and there will be sliding window of width `seq_length`. we will keep sliding the window to the right with step size = seq_length until we cover all characters. Note that characters at the end that don't fully fit one seq_length will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that yield mini-batches of seq_length.'''\n",
    "    # total chars in one batch\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr) // batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    # iterate through the array, one sequence at a time\n",
    "    # slicing window will be of seq_length width\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n + seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        \n",
    "        # Will be generator\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded_text, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 2 19 66 44  5 11 63 61 50 23]\n",
      " [45 28 82 61  5 19 66  5 61 66]\n",
      " [11 82 55 61 28 63 61 66 61  6]\n",
      " [45 61  5 19 11 61 39 19 71 11]\n",
      " [61 45 66 53 61 19 11 63 61  5]\n",
      " [39 68 45 45 71 28 82 61 66 82]\n",
      " [61 31 82 82 66 61 19 66 55 61]\n",
      " [47 69 51 28 82 45 49 80 54 61]]\n",
      "\n",
      "y\n",
      " [[19 66 44  5 11 63 61 50 23 23]\n",
      " [28 82 61  5 19 66  5 61 66  5]\n",
      " [82 55 61 28 63 61 66 61  6 28]\n",
      " [61  5 19 11 61 39 19 71 11  6]\n",
      " [45 66 53 61 19 11 63 61  5 11]\n",
      " [68 45 45 71 28 82 61 66 82 55]\n",
      " [31 82 82 66 61 19 66 55 61 45]\n",
      " [69 51 28 82 45 49 80 54 61  9]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print(f'x\\n {x[:10, :10]}\\n')\n",
    "print(f'y\\n {y[:10, :10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In `__init__` the suggested structure is as follows:\n",
    "* Define an LSTM layer that takes as params: an input size (the number of characters), a hidden layer size `n_hidden`, a number of layers `n_layers`, a dropout probability `drop_prob`, and a batch_first boolean (True, since we are batching)\n",
    "* Define a dropout layer with `dropout_prob`\n",
    "* Define a fully-connected layer with params: input size `n_hidden` and output size (the number of characters)\n",
    "* Finally, initialize the weights (again, this has been given)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can create a basic [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) as follows\n",
    "\n",
    "```python\n",
    "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "```\n",
    "\n",
    "where `input_size` is the number of characters this cell expects to see as sequential input, and `n_hidden` is the number of units in the hidden layers in the cell. And we can add dropout by adding a dropout parameter with a specified probability; this will automatically add dropout to the inputs or outputs. Finally, in the `forward` function, we can stack up the LSTM cells into layers using `.view`. With this, you pass in a list of cells and it will send the output of one cell into the next cell.\n",
    "\n",
    "We also need to create an initial hidden state of all zeros. This is done like so\n",
    "\n",
    "```python\n",
    "self.init_hidden()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With `batch_first=True`, the input will be expected to have the following dimensions: `batch x seq_length x features` where `features` here means length of one-hot vector, i.e vocabulary size.\n",
    "\n",
    "For each layer in LSTM, there will be `h_0` and `c_0` tensors which are called hidden state and cell state (short term memory cell) respectively. They will be initiated to zeros and have following dimensions: `num_layers x batch x hidden_size`\n",
    "\n",
    "Recurrent dropout used in RNNs are different than Dropout layers. They get applied to the hidden state tensors within the RNNs and don't get applied to the output tensor. They both have the same functionality why a unit gets dropped with probability `prob` using a Bernoulli random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, chars, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.chars = chars\n",
    "        \n",
    "        # Two LSTM layers stacked with dropout\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The train function gives us the ability to set the number of epochs, the learning rate, and other parameters.\n",
    "\n",
    "Below we're using an Adam optimizer and cross entropy loss since we are looking at character class scores as output. We calculate the loss and perform backpropagation, as usual!\n",
    "\n",
    "A couple of details about training: \n",
    ">* Within the batch loop, we detach the hidden state from its history; this time setting it equal to a new *tuple* variable because an LSTM has a hidden state that is a tuple of the hidden and cell states.\n",
    "* We use [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to help prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    net, data, epochs=10, batch_size=10, seq_length=50, lr=1e-3, clip=5, val_pct=0.1, print_every=10):\n",
    "\n",
    "    net.train()\n",
    "    optimizer = opt.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data) * (1 - val_pct))\n",
    "    train_data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for epoch in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        for x, y in get_batches(train_data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs.float(), h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size * seq_length))\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    output, val_h = net(inputs.float(), val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size * seq_length))\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(f'Epoch : {epoch + 1:02d}/{epochs} ... '\n",
    "                      f'Step : {counter} ... ',\n",
    "                      f'Loss : {loss.item():.4f} , Val Loss : {np.mean(val_losses):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes, and start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 \n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01/20 ... Step : 20 ...  Loss : 3.1418 , Val Loss : 3.1286\n",
      "Epoch : 01/20 ... Step : 40 ...  Loss : 3.1093 , Val Loss : 3.1187\n",
      "Epoch : 01/20 ... Step : 60 ...  Loss : 3.1159 , Val Loss : 3.1146\n",
      "Epoch : 01/20 ... Step : 80 ...  Loss : 3.1176 , Val Loss : 3.1040\n",
      "Epoch : 01/20 ... Step : 100 ...  Loss : 3.0590 , Val Loss : 3.0449\n",
      "Epoch : 01/20 ... Step : 120 ...  Loss : 2.8640 , Val Loss : 2.8341\n",
      "Epoch : 02/20 ... Step : 140 ...  Loss : 2.6406 , Val Loss : 2.5894\n",
      "Epoch : 02/20 ... Step : 160 ...  Loss : 2.5138 , Val Loss : 2.4648\n",
      "Epoch : 02/20 ... Step : 180 ...  Loss : 2.4251 , Val Loss : 2.3920\n",
      "Epoch : 02/20 ... Step : 200 ...  Loss : 2.3603 , Val Loss : 2.3285\n",
      "Epoch : 02/20 ... Step : 220 ...  Loss : 2.2783 , Val Loss : 2.2702\n",
      "Epoch : 02/20 ... Step : 240 ...  Loss : 2.2517 , Val Loss : 2.2181\n",
      "Epoch : 02/20 ... Step : 260 ...  Loss : 2.1687 , Val Loss : 2.1657\n",
      "Epoch : 03/20 ... Step : 280 ...  Loss : 2.1733 , Val Loss : 2.1222\n",
      "Epoch : 03/20 ... Step : 300 ...  Loss : 2.1069 , Val Loss : 2.0785\n",
      "Epoch : 03/20 ... Step : 320 ...  Loss : 2.0527 , Val Loss : 2.0395\n",
      "Epoch : 03/20 ... Step : 340 ...  Loss : 2.0561 , Val Loss : 2.0060\n",
      "Epoch : 03/20 ... Step : 360 ...  Loss : 1.9632 , Val Loss : 1.9781\n",
      "Epoch : 03/20 ... Step : 380 ...  Loss : 1.9648 , Val Loss : 1.9461\n",
      "Epoch : 03/20 ... Step : 400 ...  Loss : 1.9175 , Val Loss : 1.9108\n",
      "Epoch : 04/20 ... Step : 420 ...  Loss : 1.9119 , Val Loss : 1.8787\n",
      "Epoch : 04/20 ... Step : 440 ...  Loss : 1.8812 , Val Loss : 1.8524\n",
      "Epoch : 04/20 ... Step : 460 ...  Loss : 1.8116 , Val Loss : 1.8285\n",
      "Epoch : 04/20 ... Step : 480 ...  Loss : 1.8297 , Val Loss : 1.8024\n",
      "Epoch : 04/20 ... Step : 500 ...  Loss : 1.8305 , Val Loss : 1.7827\n",
      "Epoch : 04/20 ... Step : 520 ...  Loss : 1.8140 , Val Loss : 1.7589\n",
      "Epoch : 04/20 ... Step : 540 ...  Loss : 1.7328 , Val Loss : 1.7444\n",
      "Epoch : 05/20 ... Step : 560 ...  Loss : 1.7537 , Val Loss : 1.7238\n",
      "Epoch : 05/20 ... Step : 580 ...  Loss : 1.7156 , Val Loss : 1.7036\n",
      "Epoch : 05/20 ... Step : 600 ...  Loss : 1.7043 , Val Loss : 1.6907\n",
      "Epoch : 05/20 ... Step : 620 ...  Loss : 1.6953 , Val Loss : 1.6795\n",
      "Epoch : 05/20 ... Step : 640 ...  Loss : 1.6784 , Val Loss : 1.6617\n",
      "Epoch : 05/20 ... Step : 660 ...  Loss : 1.6451 , Val Loss : 1.6446\n",
      "Epoch : 05/20 ... Step : 680 ...  Loss : 1.6636 , Val Loss : 1.6331\n",
      "Epoch : 06/20 ... Step : 700 ...  Loss : 1.6412 , Val Loss : 1.6253\n",
      "Epoch : 06/20 ... Step : 720 ...  Loss : 1.6227 , Val Loss : 1.6078\n",
      "Epoch : 06/20 ... Step : 740 ...  Loss : 1.5984 , Val Loss : 1.5994\n",
      "Epoch : 06/20 ... Step : 760 ...  Loss : 1.6228 , Val Loss : 1.5880\n",
      "Epoch : 06/20 ... Step : 780 ...  Loss : 1.5846 , Val Loss : 1.5788\n",
      "Epoch : 06/20 ... Step : 800 ...  Loss : 1.5916 , Val Loss : 1.5671\n",
      "Epoch : 06/20 ... Step : 820 ...  Loss : 1.5422 , Val Loss : 1.5592\n",
      "Epoch : 07/20 ... Step : 840 ...  Loss : 1.5283 , Val Loss : 1.5527\n",
      "Epoch : 07/20 ... Step : 860 ...  Loss : 1.5422 , Val Loss : 1.5415\n",
      "Epoch : 07/20 ... Step : 880 ...  Loss : 1.5486 , Val Loss : 1.5347\n",
      "Epoch : 07/20 ... Step : 900 ...  Loss : 1.5277 , Val Loss : 1.5282\n",
      "Epoch : 07/20 ... Step : 920 ...  Loss : 1.5247 , Val Loss : 1.5226\n",
      "Epoch : 07/20 ... Step : 940 ...  Loss : 1.5171 , Val Loss : 1.5159\n",
      "Epoch : 07/20 ... Step : 960 ...  Loss : 1.5240 , Val Loss : 1.5119\n",
      "Epoch : 08/20 ... Step : 980 ...  Loss : 1.5017 , Val Loss : 1.4980\n",
      "Epoch : 08/20 ... Step : 1000 ...  Loss : 1.4899 , Val Loss : 1.4904\n",
      "Epoch : 08/20 ... Step : 1020 ...  Loss : 1.5037 , Val Loss : 1.4860\n",
      "Epoch : 08/20 ... Step : 1040 ...  Loss : 1.4958 , Val Loss : 1.4844\n",
      "Epoch : 08/20 ... Step : 1060 ...  Loss : 1.4749 , Val Loss : 1.4770\n",
      "Epoch : 08/20 ... Step : 1080 ...  Loss : 1.4753 , Val Loss : 1.4707\n",
      "Epoch : 08/20 ... Step : 1100 ...  Loss : 1.4527 , Val Loss : 1.4603\n",
      "Epoch : 09/20 ... Step : 1120 ...  Loss : 1.4632 , Val Loss : 1.4579\n",
      "Epoch : 09/20 ... Step : 1140 ...  Loss : 1.4644 , Val Loss : 1.4511\n",
      "Epoch : 09/20 ... Step : 1160 ...  Loss : 1.4354 , Val Loss : 1.4491\n",
      "Epoch : 09/20 ... Step : 1180 ...  Loss : 1.4312 , Val Loss : 1.4470\n",
      "Epoch : 09/20 ... Step : 1200 ...  Loss : 1.4146 , Val Loss : 1.4416\n",
      "Epoch : 09/20 ... Step : 1220 ...  Loss : 1.4259 , Val Loss : 1.4358\n",
      "Epoch : 09/20 ... Step : 1240 ...  Loss : 1.4126 , Val Loss : 1.4261\n",
      "Epoch : 10/20 ... Step : 1260 ...  Loss : 1.4284 , Val Loss : 1.4291\n",
      "Epoch : 10/20 ... Step : 1280 ...  Loss : 1.4339 , Val Loss : 1.4198\n",
      "Epoch : 10/20 ... Step : 1300 ...  Loss : 1.4121 , Val Loss : 1.4193\n",
      "Epoch : 10/20 ... Step : 1320 ...  Loss : 1.4020 , Val Loss : 1.4177\n",
      "Epoch : 10/20 ... Step : 1340 ...  Loss : 1.3898 , Val Loss : 1.4127\n",
      "Epoch : 10/20 ... Step : 1360 ...  Loss : 1.3892 , Val Loss : 1.4070\n",
      "Epoch : 10/20 ... Step : 1380 ...  Loss : 1.4059 , Val Loss : 1.4039\n",
      "Epoch : 11/20 ... Step : 1400 ...  Loss : 1.4189 , Val Loss : 1.4025\n",
      "Epoch : 11/20 ... Step : 1420 ...  Loss : 1.4176 , Val Loss : 1.3962\n",
      "Epoch : 11/20 ... Step : 1440 ...  Loss : 1.4083 , Val Loss : 1.3970\n",
      "Epoch : 11/20 ... Step : 1460 ...  Loss : 1.3666 , Val Loss : 1.3961\n",
      "Epoch : 11/20 ... Step : 1480 ...  Loss : 1.3768 , Val Loss : 1.3900\n",
      "Epoch : 11/20 ... Step : 1500 ...  Loss : 1.3616 , Val Loss : 1.3912\n",
      "Epoch : 11/20 ... Step : 1520 ...  Loss : 1.3827 , Val Loss : 1.3817\n",
      "Epoch : 12/20 ... Step : 1540 ...  Loss : 1.3829 , Val Loss : 1.3814\n",
      "Epoch : 12/20 ... Step : 1560 ...  Loss : 1.3936 , Val Loss : 1.3766\n",
      "Epoch : 12/20 ... Step : 1580 ...  Loss : 1.3199 , Val Loss : 1.3798\n",
      "Epoch : 12/20 ... Step : 1600 ...  Loss : 1.3362 , Val Loss : 1.3750\n",
      "Epoch : 12/20 ... Step : 1620 ...  Loss : 1.3330 , Val Loss : 1.3689\n",
      "Epoch : 12/20 ... Step : 1640 ...  Loss : 1.3269 , Val Loss : 1.3718\n",
      "Epoch : 12/20 ... Step : 1660 ...  Loss : 1.3695 , Val Loss : 1.3651\n",
      "Epoch : 13/20 ... Step : 1680 ...  Loss : 1.3465 , Val Loss : 1.3636\n",
      "Epoch : 13/20 ... Step : 1700 ...  Loss : 1.3292 , Val Loss : 1.3627\n",
      "Epoch : 13/20 ... Step : 1720 ...  Loss : 1.3092 , Val Loss : 1.3619\n",
      "Epoch : 13/20 ... Step : 1740 ...  Loss : 1.3211 , Val Loss : 1.3585\n",
      "Epoch : 13/20 ... Step : 1760 ...  Loss : 1.3184 , Val Loss : 1.3537\n",
      "Epoch : 13/20 ... Step : 1780 ...  Loss : 1.3113 , Val Loss : 1.3566\n",
      "Epoch : 13/20 ... Step : 1800 ...  Loss : 1.3233 , Val Loss : 1.3497\n",
      "Epoch : 14/20 ... Step : 1820 ...  Loss : 1.3150 , Val Loss : 1.3504\n",
      "Epoch : 14/20 ... Step : 1840 ...  Loss : 1.2781 , Val Loss : 1.3484\n",
      "Epoch : 14/20 ... Step : 1860 ...  Loss : 1.3119 , Val Loss : 1.3479\n",
      "Epoch : 14/20 ... Step : 1880 ...  Loss : 1.3239 , Val Loss : 1.3415\n",
      "Epoch : 14/20 ... Step : 1900 ...  Loss : 1.3171 , Val Loss : 1.3395\n",
      "Epoch : 14/20 ... Step : 1920 ...  Loss : 1.3010 , Val Loss : 1.3452\n",
      "Epoch : 14/20 ... Step : 1940 ...  Loss : 1.3329 , Val Loss : 1.3377\n",
      "Epoch : 15/20 ... Step : 1960 ...  Loss : 1.3002 , Val Loss : 1.3349\n",
      "Epoch : 15/20 ... Step : 1980 ...  Loss : 1.2808 , Val Loss : 1.3387\n",
      "Epoch : 15/20 ... Step : 2000 ...  Loss : 1.2681 , Val Loss : 1.3373\n",
      "Epoch : 15/20 ... Step : 2020 ...  Loss : 1.3068 , Val Loss : 1.3288\n",
      "Epoch : 15/20 ... Step : 2040 ...  Loss : 1.2854 , Val Loss : 1.3310\n",
      "Epoch : 15/20 ... Step : 2060 ...  Loss : 1.2825 , Val Loss : 1.3351\n",
      "Epoch : 15/20 ... Step : 2080 ...  Loss : 1.2850 , Val Loss : 1.3302\n",
      "Epoch : 16/20 ... Step : 2100 ...  Loss : 1.2764 , Val Loss : 1.3292\n",
      "Epoch : 16/20 ... Step : 2120 ...  Loss : 1.2901 , Val Loss : 1.3308\n",
      "Epoch : 16/20 ... Step : 2140 ...  Loss : 1.2673 , Val Loss : 1.3264\n",
      "Epoch : 16/20 ... Step : 2160 ...  Loss : 1.2734 , Val Loss : 1.3242\n",
      "Epoch : 16/20 ... Step : 2180 ...  Loss : 1.2675 , Val Loss : 1.3195\n",
      "Epoch : 16/20 ... Step : 2200 ...  Loss : 1.2600 , Val Loss : 1.3248\n",
      "Epoch : 16/20 ... Step : 2220 ...  Loss : 1.2774 , Val Loss : 1.3172\n",
      "Epoch : 17/20 ... Step : 2240 ...  Loss : 1.2656 , Val Loss : 1.3217\n",
      "Epoch : 17/20 ... Step : 2260 ...  Loss : 1.2596 , Val Loss : 1.3174\n",
      "Epoch : 17/20 ... Step : 2280 ...  Loss : 1.2717 , Val Loss : 1.3127\n",
      "Epoch : 17/20 ... Step : 2300 ...  Loss : 1.2331 , Val Loss : 1.3139\n",
      "Epoch : 17/20 ... Step : 2320 ...  Loss : 1.2528 , Val Loss : 1.3141\n",
      "Epoch : 17/20 ... Step : 2340 ...  Loss : 1.2752 , Val Loss : 1.3133\n",
      "Epoch : 17/20 ... Step : 2360 ...  Loss : 1.2769 , Val Loss : 1.3129\n",
      "Epoch : 18/20 ... Step : 2380 ...  Loss : 1.2411 , Val Loss : 1.3110\n",
      "Epoch : 18/20 ... Step : 2400 ...  Loss : 1.2554 , Val Loss : 1.3080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 18/20 ... Step : 2420 ...  Loss : 1.2501 , Val Loss : 1.3077\n",
      "Epoch : 18/20 ... Step : 2440 ...  Loss : 1.2436 , Val Loss : 1.3088\n",
      "Epoch : 18/20 ... Step : 2460 ...  Loss : 1.2500 , Val Loss : 1.3070\n",
      "Epoch : 18/20 ... Step : 2480 ...  Loss : 1.2444 , Val Loss : 1.3044\n",
      "Epoch : 18/20 ... Step : 2500 ...  Loss : 1.2306 , Val Loss : 1.3079\n",
      "Epoch : 19/20 ... Step : 2520 ...  Loss : 1.2566 , Val Loss : 1.3060\n",
      "Epoch : 19/20 ... Step : 2540 ...  Loss : 1.2583 , Val Loss : 1.3029\n",
      "Epoch : 19/20 ... Step : 2560 ...  Loss : 1.2460 , Val Loss : 1.3005\n",
      "Epoch : 19/20 ... Step : 2580 ...  Loss : 1.2659 , Val Loss : 1.3013\n",
      "Epoch : 19/20 ... Step : 2600 ...  Loss : 1.2308 , Val Loss : 1.3006\n",
      "Epoch : 19/20 ... Step : 2620 ...  Loss : 1.2142 , Val Loss : 1.3031\n",
      "Epoch : 19/20 ... Step : 2640 ...  Loss : 1.2342 , Val Loss : 1.2943\n",
      "Epoch : 20/20 ... Step : 2660 ...  Loss : 1.2471 , Val Loss : 1.2990\n",
      "Epoch : 20/20 ... Step : 2680 ...  Loss : 1.2408 , Val Loss : 1.2933\n",
      "Epoch : 20/20 ... Step : 2700 ...  Loss : 1.2320 , Val Loss : 1.2942\n",
      "Epoch : 20/20 ... Step : 2720 ...  Loss : 1.2073 , Val Loss : 1.2946\n",
      "Epoch : 20/20 ... Step : 2740 ...  Loss : 1.1995 , Val Loss : 1.2940\n",
      "Epoch : 20/20 ... Step : 2760 ...  Loss : 1.2062 , Val Loss : 1.2955\n",
      "Epoch : 20/20 ... Step : 2780 ...  Loss : 1.2677 , Val Loss : 1.2926\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net, encoded_text, n_epochs, batch_size,\n",
    "      seq_length, lr=1e-3, print_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "torch.save(checkpoint, 'char_rnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that the model is trained, we'll want to sample from it and make predictions about next characters! To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text! The output of the network is the logit so we need to use softmax to get the prob distribution over all possible characters. We can can add randomness to the model by sampling the predictions based on the probability of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        # tensor inputs\n",
    "        x = np.array([[char_to_idx[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x).to(device)\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs.float(), h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        p = p.cpu()\n",
    "        \n",
    "        # get top characters\n",
    "        # Use all character for sampling\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        # sample from top_chars using to_k\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        # p / p.sum() normalize probs in case we are sampling using top_chars\n",
    "        char = np.random.choice(top_ch, p=p / p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return idx_to_char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Priming and generating text \n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for i in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, and would have been tried an and allowing that he was saying her. He saw the simple of the first significance of her studies.\n",
      "\n",
      "\"Well, her would\n",
      "be anything to be doing.\"\n",
      "\n",
      "\"Wait a\n",
      "below, with my woman. I am saying.\n",
      "And it all more the prayer and me who has not to be all, the frost and must be asking at the\n",
      "significance of this minute, that I shall step it\n",
      "in anyone of anything. And to meet your soul of the way of me out at him and some thing in the sense. Well and there. I don't say to the same thing.\"\n",
      "\n",
      "\"An thoughts in the same talks of the meetuness of the\n",
      "sole, and with me in a person, and I am trying to see her for you with this province in his house, with her than it was a mistake, which seem and the sound of a man should have been time and a meaning. There's no\n",
      "marriage, that three men the contrary, with you, are so an official doubt and as to be,\" the same so talk to him to this well.\n",
      "\n",
      "The carriage. She tried, but shriek on a law and\n",
      "had been and her see he\n",
      "was the stear of talki\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "checkpoint = torch.load('char_rnn.pth')    \n",
    "net = CharRNN(checkpoint['tokens'],\n",
    "              n_hidden=checkpoint['n_hidden'],\n",
    "              n_layers=checkpoint['n_layers'])\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said,\n",
      "\"Yes, as the steps, went to the sockety,\" said Stepan Arkadyevitch. \"What doe nothing too,\" said Levin.\n",
      "\n",
      "\"I shall not have heard the stead. You want a petty time to bar eres, and when I was nut into a state of superfice, and I cannot carry it. Why, are it\n",
      "were, there is something beside her important.\"\n",
      "\n",
      "\"No, you'd not see\n",
      "your feelings.\"\n",
      "\n",
      "\"What do you know that her means, as that this was it.\"\n",
      "\n",
      "\"Well, will you know how to be dead? What's an and served my\n",
      "side of the morning?\"\n",
      "\n",
      "\"Well, what do I was a speak, and I do you say that I don't know. You must be an allow to the crass.\"\n",
      "\n",
      "\"I'm alreasy. I cannot come to me.\"\n",
      "\n",
      "Alexey Alexandrovitch asked Stepan\n",
      "Arkadyevitch.\n",
      "\n",
      "\"Yes, the country,\" she said, with the churk,\n",
      "and she could not take simple whee\n",
      "she had the corner when he saw with his back. He had not come off that a state of pression in a part of that her who was not her. The marsh of her propision was heard of his words. His hand.\n",
      "\n",
      "\"Yes, it was in time to discover it... Then, I'm very glad, and I'm\n",
      "going\n",
      "about it,\" said Levin, with a smelow and steppan and her\n",
      "face on\n",
      "the painter at his way and at the\n",
      "same time, she could not he say. There was nothing to talk about his son to himself. And the praise was\n",
      "crearelt, and that she was particularly time. The\n",
      "made, her\n",
      "eyes were as too happiness in the masher, who had treated her\n",
      "and stearing off her frowns of the same, so he cruened his feelings, too with the country, and at the memory of her husband's head of his hands. All the seas that was not the men with the province, while Levin told her and he came in. He saw that this wife all that seemed so that he saw that the story open of\n",
      "the party were\n",
      "still, simply broke out though the strange share, he would\n",
      "straight its to her had. The cassion of his shaking, the marsh were alone to him, and the princess walked up\n",
      "a single coat the carp to an all, and\n",
      "was in her husband's way, ald he caught into his hands of all the second strange\n",
      "and comprehension of his sister,\n",
      "took to h\n"
     ]
    }
   ],
   "source": [
    "# Print out samples\n",
    "print(sample(net, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
