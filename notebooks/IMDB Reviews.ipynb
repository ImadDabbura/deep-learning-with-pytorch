{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.txt  reviews.txt\r\n"
     ]
    }
   ],
   "source": [
    "# PATH = Path('/Users/imad/Documents/deep_learning_udacity/'\\\n",
    "#             'deep-learning-v2-pytorch/sentiment-rnn/data')\n",
    "PATH = Path('../data/imdb/')\n",
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(PATH / 'reviews.txt') as f:\n",
    "    reviews = f.read()\n",
    "with open(PATH / 'labels.txt') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "negative\n",
      "po\n"
     ]
    }
   ],
   "source": [
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.lower()\n",
    "all_text = ''.join([char for char in reviews if char not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_split = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 336713),\n",
       " ('and', 164107),\n",
       " ('a', 163009),\n",
       " ('of', 145864),\n",
       " ('to', 135720),\n",
       " ('is', 107328),\n",
       " ('br', 101872),\n",
       " ('it', 96352),\n",
       " ('in', 93968),\n",
       " ('i', 87623)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = all_text.split()\n",
    "counts = Counter(words)\n",
    "sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('the', 1),\n",
       "  ('and', 2),\n",
       "  ('a', 3),\n",
       "  ('of', 4),\n",
       "  ('to', 5),\n",
       "  ('is', 6),\n",
       "  ('br', 7),\n",
       "  ('it', 8),\n",
       "  ('in', 9),\n",
       "  ('i', 10)],\n",
       " [(1, 'the'),\n",
       "  (2, 'and'),\n",
       "  (3, 'a'),\n",
       "  (4, 'of'),\n",
       "  (5, 'to'),\n",
       "  (6, 'is'),\n",
       "  (7, 'br'),\n",
       "  (8, 'it'),\n",
       "  (9, 'in'),\n",
       "  (10, 'i')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [x[0] for x in sorted_counts] \n",
    "words_to_idx = {word:idx for idx, word in enumerate(vocab, 1)}\n",
    "idx_to_words = {idx:word for word, idx in words_to_idx.items()}\n",
    "list(words_to_idx.items())[:10], list(idx_to_words.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74072"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize reviews\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([words_to_idx[word] for word in review.split()])\n",
    "\n",
    "print(reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to ints\n",
    "labels = labels.split('\\n')\n",
    "labels = np.array([0 if label == 'negative' else 1 for label in labels])\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2514)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the min and max of all reviews\n",
    "reviews_lens = [len(review) for review in reviews_ints]\n",
    "np.min(reviews_lens), np.max(reviews_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets delete the review that is empty\n",
    "non_zero_idx = [idx for idx, review in enumerate(reviews_ints) if len(review) > 0]\n",
    "reviews_ints = np.array(reviews_ints)[non_zero_idx] #[review for review in reviews_ints if len(review) > 0]\n",
    "labels = labels[non_zero_idx]\n",
    "len(reviews_ints), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each reviews to seq_length. If a review is longer than seq_length --> truncate\n",
    "# anything to the right of seq_length. If it is shorter -- pad with zeros to the left\n",
    "def pad_sequences(reviews, seq_length):\n",
    "    padded_sequences = np.zeros((len(reviews), seq_length), dtype=int)\n",
    "\n",
    "    for i, review in enumerate(reviews):\n",
    "        padded_sequences[i, -len(review):] = np.array(review)[:seq_length]\n",
    "    \n",
    "    assert len(reviews) == len(padded_sequences)\n",
    "    assert padded_sequences.shape[1] == seq_length\n",
    "\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0, 21025,   308,     6,\n",
       "           3,  1050,   207,     8,  2138,    32,     1,   171,    57,\n",
       "          15,    49,    81,  5785,    44,   382,   110,   140,    15,\n",
       "        5194,    60,   154,     9,     1,  4975,  5852,   475,    71,\n",
       "           5,   260,    12, 21025,   308,    13,  1978,     6,    74,\n",
       "        2395,     5,   613,    73,     6,  5194,     1, 24103,     5,\n",
       "        1983, 10166,     1,  5786,  1499,    36,    51,    66,   204,\n",
       "         145,    67,  1199,  5194, 19869,     1, 37442,     4,     1,\n",
       "         221,   883,    31,  2988,    71,     4,     1,  5787,    10,\n",
       "         686,     2,    67,  1499,    54,    10,   216,     1,   383,\n",
       "           9,    62,     3,  1406,  3686,   783,     5,  3483,   180,\n",
       "           1,   382,    10,  1212, 13583,    32,   308,     3,   349,\n",
       "         341,  2913,    10,   143,   127,     5,  7690,    30,     4,\n",
       "         129,  5194,  1406,  2326,     5, 21025,   308,    10,   528,\n",
       "          12,   109,  1448,     4,    60,   543,   102,    12, 21025,\n",
       "         308,     6,   227,  4146,    48,     3,  2211,    12,     8,\n",
       "         215,    23])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 200\n",
    "padded_reviews = pad_sequences(reviews_ints, seq_length)\n",
    "padded_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_ints[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 200), (25000,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_reviews.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 200), (2500, 200), (2500, 200))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(padded_reviews, labels, test_size=0.2)\n",
    "valid_reviews, test_reviews, valid_labels, test_labels = train_test_split(test_reviews, test_labels, test_size=0.5)\n",
    "train_reviews.shape, valid_reviews.shape, test_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "train_ds = TensorDataset(torch.from_numpy(train_reviews), torch.from_numpy(train_labels))\n",
    "valid_ds = TensorDataset(torch.from_numpy(valid_reviews), torch.from_numpy(valid_labels))\n",
    "test_ds = TensorDataset(torch.from_numpy(test_reviews), torch.from_numpy(test_labels))\n",
    "\n",
    "# Create DataLoaders\n",
    "bs = 50\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1747,  1104,   885,  ...,     1,  1421,     7],\n",
       "         [    0,     0,     0,  ...,     4,    45,     4],\n",
       "         [    0,     0,     0,  ...,   958,     6,  1553],\n",
       "         ...,\n",
       "         [ 1237,    97,    39,  ...,     4,   323, 46755],\n",
       "         [   40,    26,    58,  ...,     3,  1098,  6754],\n",
       "         [   11,    18,   920,  ...,    12,   157, 11201]]),\n",
       " tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "         1, 0]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_review, sample_labels = next(iter(train_dl))\n",
    "sample_review, sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 200]), torch.Size([50]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_review.shape, sample_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, embed_dim,\n",
    "                 n_layers, hidden_size, drop_prob):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.shape[0]\n",
    "        embeddings = self.embedding(x)\n",
    "        out, hidden = self.lstm(embeddings, hidden)\n",
    "        out = self.dropout(out)\n",
    "        out = out.contiguous().view(-1, hidden_size)\n",
    "        out = torch.sigmoid(self.fc(out))\n",
    "        # Get the prediction of the last time step of each sequence\n",
    "        out = out.view(batch_size, -1)[:, -1] \n",
    "        \n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(74073, 300)\n",
       "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 300\n",
    "hidden_size = 256\n",
    "n_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, n_layers, hidden_size, drop_prob=0.5)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "epochs = 4\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = opt.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01/4 ... Step : 100 ...  Loss : 0.6679 , Val Loss : 0.6415\n",
      "Epoch : 01/4 ... Step : 200 ...  Loss : 0.6932 , Val Loss : 0.6888\n",
      "Epoch : 01/4 ... Step : 300 ...  Loss : 0.6650 , Val Loss : 0.6430\n",
      "Epoch : 01/4 ... Step : 400 ...  Loss : 0.6823 , Val Loss : 0.7057\n",
      "Epoch : 02/4 ... Step : 500 ...  Loss : 0.7153 , Val Loss : 0.7108\n",
      "Epoch : 02/4 ... Step : 600 ...  Loss : 0.5543 , Val Loss : 0.5414\n",
      "Epoch : 02/4 ... Step : 700 ...  Loss : 0.4652 , Val Loss : 0.5005\n",
      "Epoch : 02/4 ... Step : 800 ...  Loss : 0.3954 , Val Loss : 0.5317\n",
      "Epoch : 03/4 ... Step : 900 ...  Loss : 0.5296 , Val Loss : 0.4525\n",
      "Epoch : 03/4 ... Step : 1000 ...  Loss : 0.3496 , Val Loss : 0.4030\n",
      "Epoch : 03/4 ... Step : 1100 ...  Loss : 0.4184 , Val Loss : 0.3915\n",
      "Epoch : 03/4 ... Step : 1200 ...  Loss : 0.3152 , Val Loss : 0.3983\n",
      "Epoch : 04/4 ... Step : 1300 ...  Loss : 0.2261 , Val Loss : 0.4041\n",
      "Epoch : 04/4 ... Step : 1400 ...  Loss : 0.3427 , Val Loss : 0.4389\n",
      "Epoch : 04/4 ... Step : 1500 ...  Loss : 0.2650 , Val Loss : 0.4016\n",
      "Epoch : 04/4 ... Step : 1600 ...  Loss : 0.2493 , Val Loss : 0.4031\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "net.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden = net.init_hidden(bs)\n",
    "    \n",
    "    for inputs, labels in train_dl:\n",
    "        counter += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = net(inputs, hidden)\n",
    "        \n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter % 100 == 0:\n",
    "            net.eval()\n",
    "            val_hidden = net.init_hidden(bs)\n",
    "            val_losses = []\n",
    "            \n",
    "            for inputs, labels in valid_dl:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                output, val_hidden = net(inputs, val_hidden)\n",
    "                \n",
    "                val_loss = criterion(output, labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "                            \n",
    "            print(f'Epoch : {epoch + 1:02d}/{epochs} ... '\n",
    "                  f'Step : {counter} ... ',\n",
    "                  f'Loss : {loss.item():.4f} , Val Loss : {np.mean(val_losses):.4f}')\n",
    "        net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.4074\n",
      "Test accuracy : 83.88%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_hidden = net.init_hidden(bs)\n",
    "test_losses = []\n",
    "total_correct = 0\n",
    "\n",
    "for inputs, labels in test_dl:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    test_hidden = tuple([each.data for each in test_hidden])\n",
    "\n",
    "    output, test_hidden = net(inputs, test_hidden)\n",
    "\n",
    "    test_loss = criterion(output, labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    preds = output.round().long()\n",
    "    total_correct += (labels == preds).sum().item()\n",
    "\n",
    "test_loss = np.mean(test_losses)\n",
    "test_acc = total_correct / len(test_ds)\n",
    "\n",
    "print(f'Test loss : {test_loss:.4f}')\n",
    "print(f'Test accuracy : {test_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Use Pretrained Embeddings (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-25 20:13:31--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2018-12-25 20:13:31--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  93.5MB/s    in 12s     \n",
      "\n",
      "2018-12-25 20:13:43 (69.3 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'http://nlp.stanford.edu/data/glove.6B.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.100d.txt  glove.6B.200d.txt  glove.6B.300d.txt  glove.6B.50d.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load Embedding with 300 embedding dimensions\n",
    "embeddings_index = {}\n",
    "with open('../data/glove/glove.6B.300d.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        data = line.split()\n",
    "        word = data[0]\n",
    "        values = np.array(data[1:]).astype(float)\n",
    "        embeddings_index[word] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.6560e-02,  2.1318e-01, -7.4364e-03, -4.5854e-01, -3.5639e-02,\n",
       "        2.3643e-01, -2.8836e-01,  2.1521e-01, -1.3486e-01, -1.6413e+00,\n",
       "       -2.6091e-01,  3.2434e-02,  5.6621e-02, -4.3296e-02, -2.1672e-02,\n",
       "        2.2476e-01, -7.5129e-02, -6.7018e-02, -1.4247e-01,  3.8825e-02,\n",
       "       -1.8951e-01,  2.9977e-01,  3.9305e-01,  1.7887e-01, -1.7343e-01,\n",
       "       -2.1178e-01,  2.3617e-01, -6.3681e-02, -4.2318e-01, -1.1661e-01,\n",
       "        9.3754e-02,  1.7296e-01, -3.3073e-01,  4.9112e-01, -6.8995e-01,\n",
       "       -9.2462e-02,  2.4742e-01, -1.7991e-01,  9.7908e-02,  8.3118e-02,\n",
       "        1.5299e-01, -2.7276e-01, -3.8934e-02,  5.4453e-01,  5.3737e-01,\n",
       "        2.9105e-01, -7.3514e-03,  4.7880e-02, -4.0760e-01, -2.6759e-02,\n",
       "        1.7919e-01,  1.0977e-02, -1.0963e-01, -2.6395e-01,  7.3990e-02,\n",
       "        2.6236e-01, -1.5080e-01,  3.4623e-01,  2.5758e-01,  1.1971e-01,\n",
       "       -3.7135e-02, -7.1593e-02,  4.3898e-01, -4.0764e-02,  1.6425e-02,\n",
       "       -4.4640e-01,  1.7197e-01,  4.6246e-02,  5.8639e-02,  4.1499e-02,\n",
       "        5.3948e-01,  5.2495e-01,  1.1361e-01, -4.8315e-02, -3.6385e-01,\n",
       "        1.8704e-01,  9.2761e-02, -1.1129e-01, -4.2085e-01,  1.3992e-01,\n",
       "       -3.9338e-01, -6.7945e-02,  1.2188e-01,  1.6707e-01,  7.5169e-02,\n",
       "       -1.5529e-02, -1.9499e-01,  1.9638e-01,  5.3194e-02,  2.5170e-01,\n",
       "       -3.4845e-01, -1.0638e-01, -3.4692e-01, -1.9024e-01, -2.0040e-01,\n",
       "        1.2154e-01, -2.9208e-01,  2.3353e-02, -1.1618e-01, -3.5768e-01,\n",
       "        6.2304e-02,  3.5884e-01,  2.9060e-02,  7.3005e-03,  4.9482e-03,\n",
       "       -1.5048e-01, -1.2313e-01,  1.9337e-01,  1.2173e-01,  4.4503e-01,\n",
       "        2.5147e-01,  1.0781e-01, -1.7716e-01,  3.8691e-02,  8.1530e-02,\n",
       "        1.4667e-01,  6.3666e-02,  6.1332e-02, -7.5569e-02, -3.7724e-01,\n",
       "        1.5850e-02, -3.0342e-01,  2.8374e-01, -4.2013e-02, -4.0715e-02,\n",
       "       -1.5269e-01,  7.4980e-02,  1.5577e-01,  1.0433e-01,  3.1393e-01,\n",
       "        1.9309e-01,  1.9429e-01,  1.5185e-01, -1.0192e-01, -1.8785e-02,\n",
       "        2.0791e-01,  1.3366e-01,  1.9038e-01, -2.5558e-01,  3.0400e-01,\n",
       "       -1.8960e-02,  2.0147e-01, -4.2110e-01, -7.5156e-03, -2.7977e-01,\n",
       "       -1.9314e-01,  4.6204e-02,  1.9971e-01, -3.0207e-01,  2.5735e-01,\n",
       "        6.8107e-01, -1.9409e-01,  2.3984e-01,  2.2493e-01,  6.5224e-01,\n",
       "       -1.3561e-01, -1.7383e-01, -4.8209e-02, -1.1860e-01,  2.1588e-03,\n",
       "       -1.9525e-02,  1.1948e-01,  1.9346e-01, -4.0820e-01, -8.2966e-02,\n",
       "        1.6626e-01, -1.0601e-01,  3.5861e-01,  1.6922e-01,  7.2590e-02,\n",
       "       -2.4803e-01, -1.0024e-01, -5.2491e-01, -1.7745e-01, -3.6647e-01,\n",
       "        2.6180e-01, -1.2077e-02,  8.3190e-02, -2.1528e-01,  4.1045e-01,\n",
       "        2.9136e-01,  3.0869e-01,  7.8864e-02,  3.2207e-01, -4.1023e-02,\n",
       "       -1.0970e-01, -9.2041e-02, -1.2339e-01, -1.6416e-01,  3.5382e-01,\n",
       "       -8.2774e-02,  3.3171e-01, -2.4738e-01, -4.8928e-02,  1.5746e-01,\n",
       "        1.8988e-01, -2.6642e-02,  6.3315e-02, -1.0673e-02,  3.4089e-01,\n",
       "        1.4106e+00,  1.3417e-01,  2.8191e-01, -2.5940e-01,  5.5267e-02,\n",
       "       -5.2425e-02, -2.5789e-01,  1.9127e-02, -2.2084e-02,  3.2113e-01,\n",
       "        6.8818e-02,  5.1207e-01,  1.6478e-01, -2.0194e-01,  2.9232e-01,\n",
       "        9.8575e-02,  1.3145e-02, -1.0652e-01,  1.3510e-01, -4.5332e-02,\n",
       "        2.0697e-01, -4.8425e-01, -4.4706e-01,  3.3305e-03,  2.9264e-03,\n",
       "       -1.0975e-01, -2.3325e-01,  2.2442e-01, -1.0503e-01,  1.2339e-01,\n",
       "        1.0978e-01,  4.8994e-02, -2.5157e-01,  4.0319e-01,  3.5318e-01,\n",
       "        1.8651e-01, -2.3622e-02, -1.2734e-01,  1.1475e-01,  2.7359e-01,\n",
       "       -2.1866e-01,  1.5794e-02,  8.1754e-01, -2.3792e-02, -8.5469e-01,\n",
       "       -1.6203e-01,  1.8076e-01,  2.8014e-02, -1.4340e-01,  1.3139e-03,\n",
       "       -9.1735e-02, -8.9704e-02,  1.1105e-01, -1.6703e-01,  6.8377e-02,\n",
       "       -8.7388e-02, -3.9789e-02,  1.4184e-02,  2.1187e-01,  2.8579e-01,\n",
       "       -2.8797e-01, -5.8996e-02, -3.2436e-02, -4.7009e-03, -1.7052e-01,\n",
       "       -3.4741e-02, -1.1489e-01,  7.5093e-02,  9.9526e-02,  4.8183e-02,\n",
       "       -7.3775e-02, -4.1817e-01,  4.1268e-03,  4.4414e-01, -1.6062e-01,\n",
       "        1.4294e-01, -2.2628e+00, -2.7347e-02,  8.1311e-01,  7.7417e-01,\n",
       "       -2.5639e-01, -1.1576e-01, -1.1982e-01, -2.1363e-01,  2.8429e-02,\n",
       "        2.7261e-01,  3.1026e-02,  9.6782e-02,  6.7769e-03,  1.4082e-01,\n",
       "       -1.3064e-02, -2.9686e-01, -7.9913e-02,  1.9500e-01,  3.1549e-02,\n",
       "        2.8506e-01, -8.7461e-02,  9.0611e-03, -2.0989e-01,  5.3913e-02])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embeddings_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in words_to_idx.items():\n",
    "    embeddings_vector = embeddings_index.get(word)\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i - 1] = embeddings_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74073, 300)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(74073, 300)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_layer = nn.Embedding(vocab_size, 300)\n",
    "emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb_layer.load_state_dict({'weight': torch.from_numpy(embeddings_matrix)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
       "        [ 0.0385, -0.0398,  0.0827,  ..., -0.3343,  0.0118,  0.0597],\n",
       "        [-0.2971,  0.0940, -0.0967,  ...,  0.0597, -0.2285,  0.2960],\n",
       "        ...,\n",
       "        [ 0.0585,  0.9133, -0.0220,  ...,  0.2419,  0.1127, -0.2170],\n",
       "        [-0.1070, -0.2626,  0.6227,  ...,  0.0524, -0.1153,  0.1335],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_layer.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04656  ,  0.21318  , -0.0074364, ...,  0.0090611, -0.20989  ,\n",
       "         0.053913 ],\n",
       "       [ 0.038466 , -0.039792 ,  0.082747 , ..., -0.33427  ,  0.011807 ,\n",
       "         0.059703 ],\n",
       "       [-0.29712  ,  0.094049 , -0.096662 , ...,  0.059717 , -0.22853  ,\n",
       "         0.29602  ],\n",
       "       ...,\n",
       "       [ 0.058459 ,  0.91331  , -0.022032 , ...,  0.24188  ,  0.11273  ,\n",
       "        -0.21702  ],\n",
       "       [-0.10701  , -0.26264  ,  0.6227   , ...,  0.052448 , -0.11528  ,\n",
       "         0.13352  ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74073, 300])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_layer.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_embed_layer(num_embeddings, embedding_dim, embeddings_matrix, trainable=False):\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': torch.from_numpy(embeddings_matrix)})\n",
    "    \n",
    "    if not trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    \n",
    "    return emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, embed_dim,\n",
    "                 n_layers, hidden_size, drop_prob, embeddings_matrix):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = create_embed_layer(input_size, embed_dim, embeddings_matrix)\n",
    "        self.lstm = nn.LSTM(embed_dim, self.hidden_size, self.n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(self.hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.shape[0]\n",
    "        embeddings = self.embedding(x)\n",
    "        out, hidden = self.lstm(embeddings, hidden)\n",
    "        out = self.dropout(out)\n",
    "        out = out.contiguous().view(-1, hidden_size)\n",
    "        out = torch.sigmoid(self.fc(out))\n",
    "        # Get the prediction of the last time step of each sequence\n",
    "        out = out.view(batch_size, -1)[:, -1] \n",
    "        \n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(74073, 300)\n",
       "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 300\n",
    "hidden_size = 256\n",
    "n_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim,\n",
    "                   n_layers, hidden_size, drop_prob=0.5,\n",
    "                   embeddings_matrix=embeddings_matrix)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.embedding.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "epochs = 8\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = opt.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01/8 ... Step : 100 ...  Loss : 0.6765 , Val Loss : 0.6925\n",
      "Epoch : 01/8 ... Step : 200 ...  Loss : 0.6739 , Val Loss : 0.6936\n",
      "Epoch : 01/8 ... Step : 300 ...  Loss : 0.7045 , Val Loss : 0.6833\n",
      "Epoch : 01/8 ... Step : 400 ...  Loss : 0.6743 , Val Loss : 0.6713\n",
      "Epoch : 02/8 ... Step : 500 ...  Loss : 0.6470 , Val Loss : 0.6743\n",
      "Epoch : 02/8 ... Step : 600 ...  Loss : 0.5625 , Val Loss : 0.7115\n",
      "Epoch : 02/8 ... Step : 700 ...  Loss : 0.6412 , Val Loss : 0.6475\n",
      "Epoch : 02/8 ... Step : 800 ...  Loss : 0.6501 , Val Loss : 0.6712\n",
      "Epoch : 03/8 ... Step : 900 ...  Loss : 0.6229 , Val Loss : 0.6321\n",
      "Epoch : 03/8 ... Step : 1000 ...  Loss : 0.5135 , Val Loss : 0.6168\n",
      "Epoch : 03/8 ... Step : 1100 ...  Loss : 0.5657 , Val Loss : 0.6201\n",
      "Epoch : 03/8 ... Step : 1200 ...  Loss : 0.7493 , Val Loss : 0.5753\n",
      "Epoch : 04/8 ... Step : 1300 ...  Loss : 0.5506 , Val Loss : 0.5724\n",
      "Epoch : 04/8 ... Step : 1400 ...  Loss : 0.4530 , Val Loss : 0.5448\n",
      "Epoch : 04/8 ... Step : 1500 ...  Loss : 0.4838 , Val Loss : 0.5690\n",
      "Epoch : 04/8 ... Step : 1600 ...  Loss : 0.4111 , Val Loss : 0.5147\n",
      "Epoch : 05/8 ... Step : 1700 ...  Loss : 0.4294 , Val Loss : 0.5244\n",
      "Epoch : 05/8 ... Step : 1800 ...  Loss : 0.3032 , Val Loss : 0.5148\n",
      "Epoch : 05/8 ... Step : 1900 ...  Loss : 0.2869 , Val Loss : 0.5060\n",
      "Epoch : 05/8 ... Step : 2000 ...  Loss : 0.4362 , Val Loss : 0.5057\n",
      "Epoch : 06/8 ... Step : 2100 ...  Loss : 0.2834 , Val Loss : 0.5459\n",
      "Epoch : 06/8 ... Step : 2200 ...  Loss : 0.3634 , Val Loss : 0.5702\n",
      "Epoch : 06/8 ... Step : 2300 ...  Loss : 0.3806 , Val Loss : 0.5369\n",
      "Epoch : 06/8 ... Step : 2400 ...  Loss : 0.2657 , Val Loss : 0.4974\n",
      "Epoch : 07/8 ... Step : 2500 ...  Loss : 0.2024 , Val Loss : 0.5956\n",
      "Epoch : 07/8 ... Step : 2600 ...  Loss : 0.2267 , Val Loss : 0.5804\n",
      "Epoch : 07/8 ... Step : 2700 ...  Loss : 0.1739 , Val Loss : 0.5546\n",
      "Epoch : 07/8 ... Step : 2800 ...  Loss : 0.1466 , Val Loss : 0.5941\n",
      "Epoch : 08/8 ... Step : 2900 ...  Loss : 0.2023 , Val Loss : 0.6906\n",
      "Epoch : 08/8 ... Step : 3000 ...  Loss : 0.2242 , Val Loss : 0.6548\n",
      "Epoch : 08/8 ... Step : 3100 ...  Loss : 0.1143 , Val Loss : 0.6344\n",
      "Epoch : 08/8 ... Step : 3200 ...  Loss : 0.1196 , Val Loss : 0.6339\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "net.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden = net.init_hidden(bs)\n",
    "    \n",
    "    for inputs, labels in train_dl:\n",
    "        counter += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = net(inputs, hidden)\n",
    "        \n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter % 100 == 0:\n",
    "            net.eval()\n",
    "            val_hidden = net.init_hidden(bs)\n",
    "            val_losses = []\n",
    "            \n",
    "            for inputs, labels in valid_dl:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                output, val_hidden = net(inputs, val_hidden)\n",
    "                \n",
    "                val_loss = criterion(output, labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "                            \n",
    "            print(f'Epoch : {epoch + 1:02d}/{epochs} ... '\n",
    "                  f'Step : {counter} ... ',\n",
    "                  f'Loss : {loss.item():.4f} , Val Loss : {np.mean(val_losses):.4f}')\n",
    "        net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.5580\n",
      "Test accuracy : 80.40%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_hidden = net.init_hidden(bs)\n",
    "test_losses = []\n",
    "total_correct = 0\n",
    "\n",
    "for inputs, labels in test_dl:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    test_hidden = tuple([each.data for each in test_hidden])\n",
    "\n",
    "    output, test_hidden = net(inputs, test_hidden)\n",
    "\n",
    "    test_loss = criterion(output, labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    preds = output.round().long()\n",
    "    total_correct += (labels == preds).sum().item()\n",
    "\n",
    "test_loss = np.mean(test_losses)\n",
    "test_acc = total_correct / len(test_ds)\n",
    "\n",
    "print(f'Test loss : {test_loss:.4f}')\n",
    "print(f'Test accuracy : {test_acc:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
